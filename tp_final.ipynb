{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo práctico final - NLP\n",
    "\n",
    "## Alumno: Garay Gorta, Isaías.\n",
    "\n",
    "El tema elegido es el lenguaje *Python*, para esto se utilizará la documentación oficial de *Python* y un dataset de *Kaggle* con los proyectos más populares para este lenguaje en el año 2023.\n",
    "\n",
    "- [Documentación Python](https://docs.python.org/es/3/download.html)\n",
    "- [Dataset Kaggle](https://www.kaggle.com/datasets/yeoyunsianggeremie/most-popular-python-projects-on-github-2018-2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1 - RAG\n",
    "\n",
    "Crear un chatbot experto en un tema a elección, usando la técnica RAG (Retrieval Augmented Generation).\n",
    "Como fuentes de conocimiento se utilizarán al menos las siguientes fuentes:\n",
    "- Documentos de texto\n",
    "- Datos numéricos en formato tabular (por ej., Dataframes, CSV, sqlite, etc.)\n",
    "- Base de datos de grafos (Online o local)\n",
    "\n",
    "El sistema debe poder llevar a cabo una conversación en lenguaje español. El usuario podrá hacer preguntas, que el chatbot intentará responder a partir de datos de algunas de sus fuentes. El asistente debe poder clasificar las preguntas, para saber qué fuentes de datos utilizar como contexto para generar una respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-01-26 21:31:47.013954: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-26 21:31:47.014001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-26 21:31:47.098984: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-26 21:31:47.272649: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-26 21:31:48.458644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package stopwords to /home/isaias/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/isaias/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import unicodedata\n",
    "import chromadb\n",
    "import gdown\n",
    "import fitz\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos los archivos necesarios si no existe la carpeta `datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_DRIVE = 'https://drive.google.com/drive/folders/15cRVZcyHEJqSCgwMjdiFvFM9LFHTetbv?usp=drive_link'\n",
    "\n",
    "if not os.path.exists('datasets/'):\n",
    "    gdown.download_folder(URL_DRIVE, quiet=True, output='datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos tabulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_proyectos_python = pd.read_csv('datasets/popular_python_projects_2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de datos vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "def texto_de_pdf(pdf_path):\n",
    "  '''\n",
    "  Recibe el path a un archivo pdf y devuelve \n",
    "  su contenido en un string.\n",
    "  '''\n",
    "  texto = str()\n",
    "\n",
    "  doc = fitz.open(pdf_path)\n",
    "  for page in doc:\n",
    "    texto += page.get_text()\n",
    "\n",
    "  return texto\n",
    "\n",
    "def limpiar_texto(string):\n",
    "  '''\n",
    "  Recibe un string, lo pasa a minúscula\n",
    "  y elimina un formato que aparece en el índice\n",
    "  '''\n",
    "  return string.replace(' .', '').lower()\n",
    "\n",
    "def quitar_tildes(input_str):\n",
    "  '''\n",
    "  Recibe un string y elimina las tildes\n",
    "  '''\n",
    "  \n",
    "  nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "  return ''.join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "def quitar_stopwords(text):\n",
    "  '''\n",
    "  Recibe un string y elimina las stopswords\n",
    "  '''\n",
    "\n",
    "  word_tokens = word_tokenize(text)\n",
    "  filtered_text = [word for word in word_tokens if word.casefold() not in stop_words]\n",
    "  return ' '.join(filtered_text)\n",
    "\n",
    "def procesar_texto(pdf_path):\n",
    "  '''\n",
    "  Recibe el path a un pdf y aplica la \n",
    "  normalización a su texto\n",
    "  '''\n",
    "\n",
    "  texto = texto_de_pdf(pdf_path)\n",
    "  texto = limpiar_texto(texto)\n",
    "  texto = quitar_tildes(texto)\n",
    "  texto = quitar_stopwords(texto)\n",
    "\n",
    "  return texto\n",
    "\n",
    "def dict_texto(directorio_pdf):\n",
    "    '''\n",
    "    Recibe el nombre de un directorio, devuelve un diccionario\n",
    "    donde la clave es el nombre del archivo pdf y su valor es\n",
    "    una lista con chunks del texto procesado del pdf.\n",
    "    '''\n",
    "\n",
    "    dict_pdf = dict()\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=150)\n",
    "\n",
    "    for file in os.listdir(directorio_pdf):\n",
    "        if not file.endswith('.pdf'):\n",
    "            continue\n",
    "        \n",
    "        path_pdf = f'{directorio_pdf}/{file}'\n",
    "\n",
    "        txt = splitter.split_text(procesar_texto(path_pdf))\n",
    "\n",
    "        dict_pdf[file] = txt\n",
    "\n",
    "    return dict_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el texto de los archivos con las funciones definidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pdfs = dict_texto('datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a vectorizar el texto con un modelo de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual/3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además creamos la instancia de la base de datos `ChromaDB` donde vamos a almacenar los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "collection = client.get_or_create_collection('documentacion_python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 21:32:40.323300: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 314593280 exceeds 10% of free system memory.\n",
      "2024-01-26 21:32:40.370318: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 157296640 exceeds 10% of free system memory.\n",
      "2024-01-26 21:32:40.370355: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 157296640 exceeds 10% of free system memory.\n",
      "2024-01-26 21:32:40.370373: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 157296640 exceeds 10% of free system memory.\n",
      "2024-01-26 21:32:40.617842: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 157296640 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "def almacenar_texto(dicc, collection):\n",
    "    '''\n",
    "    Recibe un diccionario y una colección para almacenar\n",
    "    embeddings, itera sobre cada key del diccionario, vectoriza\n",
    "    los chunks de texto y los almacena en la colección.\n",
    "    '''\n",
    "    for key in dicc.keys():\n",
    "        embeddings = embed(dicc[key]).numpy().tolist()\n",
    "        chunks = dicc[key]\n",
    "        ids = [f'documento_{key}-parte{i}' \n",
    "               for i in range(1, len(chunks) + 1)]\n",
    "        \n",
    "        collection.add(\n",
    "                documents=chunks,\n",
    "                embeddings=embeddings,\n",
    "                ids=ids)\n",
    "        \n",
    "almacenar_texto(dict_pdfs, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de datos de grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fuente de estos datos será la base de datos online DBpedia. Se va a usar para conceptos general de programación que no se encuentren en los embeddings de los pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No puede encontrar nada en DBpedia sobre: 'Que es JavaSript?'\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "def ejecutar_consulta_sparql(consulta):\n",
    "    '''\n",
    "    Ejecuta una consulta SPARQL en la base de datos DBpedia en español.\n",
    "    '''\n",
    "    sparql = SPARQLWrapper(\"http://es.dbpedia.org/sparql\")\n",
    "    sparql.setQuery(consulta)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    resultados = sparql.query().convert()\n",
    "    return resultados\n",
    "\n",
    "def extraer_concepto_programacion(consulta):\n",
    "    '''\n",
    "    Función para extraer el concpeto de programación de una consulta.\n",
    "    '''\n",
    "\n",
    "    nlp = spacy.load('es_core_news_lg')\n",
    "    texto = nlp(consulta)\n",
    "\n",
    "    resultado = str()\n",
    "\n",
    "    # Si se encuentre al menos una entidad\n",
    "    # en la consulta se devuelve la primera.\n",
    "    if texto.ents:\n",
    "        resultado = texto.ents[0]\n",
    "\n",
    "    return resultado\n",
    "\n",
    "def consultar_dbpedia(consulta):\n",
    "    '''\n",
    "    Consulta la base de datos DBpedia en español para obtener\n",
    "    información sobre un concepto de programación específico.\n",
    "    '''\n",
    "\n",
    "    concepto = extraer_concepto_programacion(consulta)\n",
    "\n",
    "    consulta_sparql = f'''\n",
    "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    \n",
    "    SELECT ?resource ?label ?abstract ?tipo\n",
    "    WHERE {{\n",
    "        ?resource a dbo:ProgrammingLanguage ;\n",
    "                  rdfs:label ?label ;\n",
    "                  dbo:abstract ?abstract .\n",
    "        FILTER(LANG(?label) = \"es\" && LANG(?abstract) = \"es\" && CONTAINS(?label, \"{concepto}\"))\n",
    "        OPTIONAL {{ ?resource rdf:type ?tipo }}\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    '''\n",
    "\n",
    "    resultados = ejecutar_consulta_sparql(consulta_sparql)\n",
    "\n",
    "    if concepto and resultados['results']['bindings']:\n",
    "        concepto = resultados['results']['bindings'][0]['abstract']['value']\n",
    "    else:\n",
    "        concepto = f'No puede encontrar nada en DBpedia sobre: \\'{consulta}\\''\n",
    "\n",
    "    return concepto\n",
    "\n",
    "# Ejemplo de uso:\n",
    "ejemplo_consulta = 'Que es JavaSript?'\n",
    "consultar_dbpedia(ejemplo_consulta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elección de la fuente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se crea una función que recibirá una consulta del usuario y decidirá cuál de las fuentes es más probable que pueda llegar a responder la consulta. Para se utiliza un modelo pre-entrenado para la generación de embeddings de oraciones utilizando `SentenceTransformer` con el modelo `paraphrase-MiniLM-L6-v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Cargamos el modelo\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "consultas_ejemplo = {\n",
    "    'grafo': ['Qué es JavaScript?'],\n",
    "    'csv': ['Proyectos populares Python'],\n",
    "    'vectores': ['Qué es Python?']\n",
    "}\n",
    "\n",
    "def determinar_fuente(consulta, consultas_ejemplo):\n",
    "    '''\n",
    "    Determina la fuente más probable para responder\n",
    "    a una consulta mediante búsqueda de pocos disparos.\n",
    "    '''\n",
    "\n",
    "    # Umbral de similitud\n",
    "    UMBRAL_SIMILITUD = 0.3\n",
    "\n",
    "    # Generamos la representación embebida de la consulta del usuario\n",
    "    embedding_consulta = model.encode(consulta, convert_to_tensor=True)\n",
    "\n",
    "    # Obtenemos representaciones embebidas de los ejemplos de pocos disparos\n",
    "    ejemplos_embeddings = dict()\n",
    "    for fuente, ejemplos in consultas_ejemplo.items():\n",
    "        ejemplos_embeddings[fuente] = model.encode(ejemplos,\n",
    "                                                   convert_to_tensor=True)\n",
    "\n",
    "    # Se calcula similitud coseno entre la consulta y cada conjunto de ejemplos\n",
    "    similitudes = dict()\n",
    "    for fuente, embeddings in ejemplos_embeddings.items():\n",
    "        similitudes[fuente] = util.pytorch_cos_sim(embedding_consulta,\n",
    "                                                   embeddings).mean().item()\n",
    "\n",
    "    # Finalmente se devuelve la fuente de datos con la similitud más alta\n",
    "    fuente_probable = max(similitudes, key=similitudes.get)\n",
    "    if similitudes[fuente_probable] < UMBRAL_SIMILITUD:\n",
    "      fuente_probable = str()\n",
    "    return fuente_probable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from jinja2 import Template\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_respuesta(prompt, max_new_tokens=768):\n",
    "  try:\n",
    "    api_key = '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'\n",
    "    api_url = 'https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta'\n",
    "\n",
    "    headers = None\n",
    "    if api_key:\n",
    "      headers = {'Authorization': f'Bearer {api_key}'}\n",
    "\n",
    "    data = {\n",
    "    'inputs': prompt,\n",
    "    'parameters': {\n",
    "    'max_new_tokens': max_new_tokens,\n",
    "    'temperature': 0.7,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.95}\n",
    "    }\n",
    "    \n",
    "    if headers:\n",
    "      # Realizamos la solicitud POST\n",
    "      response = requests.post(api_url, headers=headers, json=data)\n",
    "      # Se extrae la respuesta\n",
    "      respuesta = response.json()[0]['generated_text'][len(prompt):]\n",
    "      return respuesta\n",
    "  except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_jira(messages, add_generation_prompt=True):\n",
    "  \n",
    "  # Definimos la plantilla jinja\n",
    "  template_str = \"{% for message in messages %}\"\n",
    "  template_str += \"{% if message['role'] == 'user' %}\"\n",
    "  template_str += \"<|user|>{{ message['content'] }}</s>\\n\"\n",
    "  template_str += \"{% elif message['role'] == 'assistant' %}\"\n",
    "  template_str += \"<|assistant|>{{ message['content'] }}</s>\\n\"\n",
    "  template_str += \"{% elif message['role'] == 'system' %}\"\n",
    "  template_str += \"<|system|>{{ message['content'] }}</s>\\n\"\n",
    "  template_str += \"{% else %}\"\n",
    "  template_str += \"<|unknown|>{{ message['content'] }}</s>\\n\"\n",
    "  template_str += \"{% endif %}\"\n",
    "  template_str += \"{% endfor %}\"\n",
    "  template_str += \"{% if add_generation_prompt %}\"\n",
    "  template_str += \"<|assistant|>\\n\"\n",
    "  template_str += \"{% endif %}\"\n",
    "  \n",
    "  # Se crea una instancia de plantilla con el string\n",
    "  template = Template(template_str)\n",
    "\n",
    "  # Renderizar la plantilla con los mensajes proporcionados\n",
    "  return template.render(messages=messages,\n",
    "                         add_generation_prompt=add_generation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_prompt(query_str, context_info):\n",
    "  '''\n",
    "  Función que recibe la pregunta del usuario y el contexto\n",
    "  que puede responder esa pregunta para armar el prompt \n",
    "  que usará el modelo.\n",
    "  '''\n",
    "\n",
    "  prompt = (\n",
    "  'La información de contexto es la siguiente:\\n'\n",
    "  '--------------------------\\n'\n",
    "  '{context_str}\\n'\n",
    "  '--------------------------\\n'\n",
    "  'Dada la información de contexto anterior, y sin utilizar conocimiento previo, responde en español la siguiente pregunta.\\n'\n",
    "  'Pregunta: {query_str}\\n'\n",
    "  'Respuesta: '\n",
    "  )\n",
    "\n",
    "  context_str = context_info\n",
    "  messages = [\n",
    "  {\n",
    "  'role': 'system',\n",
    "  'content': 'Eres un asistente útil que siempre responde con respuestas veraces, útiles y basadas en hechos.',\n",
    "  },\n",
    "  {'role': 'user', 'content': prompt.format(context_str=context_str, query_str=query_str)},\n",
    "  ]\n",
    "  final_prompt = template_jira(messages)\n",
    "  return final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contexto_fuente_externa(fuente, consulta):\n",
    "  '''\n",
    "  Recibe la consulta del usuario y determine que \n",
    "  fuente de información es pertinente pare responerla.\n",
    "  '''\n",
    "  context_info = str()\n",
    "\n",
    "  if fuente == 'csv':\n",
    "    context_info = df_proyectos_python\n",
    "  elif fuente == 'grafo':\n",
    "    info_dbpedia = consultar_dbpedia(consulta)\n",
    "    context_info += f'{info_dbpedia}\\n'\n",
    "  elif fuente == 'vectores':\n",
    "    embedding_consulta = embed([consulta]).numpy().tolist()\n",
    "    results = collection.query(\n",
    "                query_embeddings=embedding_consulta,\n",
    "                n_results=3 )\n",
    "    \n",
    "    if results['documents']:\n",
    "      for result in results['documents']:\n",
    "        for text in result:\n",
    "          context_info += f'{text}\\n'\n",
    "  \n",
    "  return context_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuente de contexto seleccionada: vectores\n",
      "Pregunta: Qué es Python?\n",
      "Respuesta:\n",
      "Python es un lenguaje de programación utilizado en varios países alrededor del mundo, cuyo nombre fue sugerido por su creador, Guido van Rossum, debido a su similitud con el nombre de la popular serie de comedia británica Monty Python's Flying Circus. Python es utilizado en organizaciones de alto perfil, incluyendo el servidor de aplicaciones Zope y la lista de correo electrónico Mailman, y se encuentra presente en varias distribuciones de Linux, como Red Hat. Existe un gran número de líneas de código Python en todo el mundo, y cualquier cambio en el lenguaje podría invalidar programas más pequeños, aunque eso es raro en proporción menor. Python también tiene una comunidad activa que se reúne en listas de correo y discute nuevos desarrollos en el foro de Python (python-dev). La Python Software Foundation se encarga de la infraestructura de proyectos de Python ubicada en diferentes partes del mundo, y se puede encontrar en el sitio web www.python.org.\n",
      "-------------------------------------------------------\n",
      "Fuente de contexto seleccionada: csv\n",
      "Pregunta: Top 5 proyectos de Python en 2023\n",
      "Respuesta:\n",
      "Según el archivo CSV proporcionado, aquí están los cinco proyectos de Python más populares en 2023, según el número de estrellas y forks en su repositorio en GitHub:\n",
      "\n",
      "1. Python Awesome - Esta es una lista curada de maravillosos frameworks Python, bibliotecas, herramientas y recursos para aprender Python. Con más de 151.000 estrellas y 22.357 forks, es seguramente uno de los proyectos de Python más populares en este momento.\n",
      "\n",
      "2. Youtube-dl - Un programa de línea de comandos para descargar videos de YouTube y otros sitios web. Con más de 116.000 estrellas y 8.482 forks, es un proyecto de Python muy utilizado.\n",
      "\n",
      "3. Core - Este es el repositorio principal de Home Assistant, una plataforma de automatización de hogar abierta y gratuita. Con más de 56.901 estrellas y 21.218 forks, es una de las principales contribuciones a la comunidad de Python.\n",
      "\n",
      "4. Keras - Una biblioteca de aprendizaje automático para Python, construida sobre TensorFlow. Con más de 56.901 estrellas y 19.245 forks, es un proyecto de Python muy activo y popular.\n",
      "\n",
      "5. Ansible - Una herramienta de automatización IT radicalmente simple. Con más de 55.872 estrellas y 22.807 forks, es una herramienta de Python muy popular en la comunidad de IT y DevOps.\n",
      "\n",
      "Espero que esto ayude en la búsqueda de proyectos de Python populares en 2023!\n",
      "-------------------------------------------------------\n",
      "Fuente de contexto seleccionada: vectores\n",
      "Pregunta: Qué es Java?\n",
      "Respuesta:\n",
      "Java es un lenguaje de programación orientado a objetos que sigue estándares de codificación establecidos por la compañía Oracle. En Java, los atributos se prefieren con la letra M_ para ser más explícitos y se utilizan declaraciones de variables locales. Cuando se hace referencia explícita a un método de una clase particular, se debe usar el operador : : en C++, o escribir baseclass.metodoname() en Python. Además, en Java se pueden definir datos estáticos tanto para las clases como para los métodos estáticos. En resumen, Java es un lenguaje de programación popular en el campo de la informática, conocido por su seguridad y su aplicación en la creación de aplicaciones empresariales y web.\n",
      "-------------------------------------------------------\n",
      "Fuente de contexto seleccionada: vectores\n",
      "Pregunta: Cómo estás?\n",
      "Respuesta:\n",
      "Respuesta: Estoy bien gracias por preguntar. Como soy un asistente automático, no tengo un estado físico y por lo tanto no me puede preguntar cómo estoy en un sentido literal. Pero en cualquier caso, estoy listo y disponible para ayudarle con cualquier consulta que tenga.\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "consultas = ['Qué es Python?', 'Top 5 proyectos de Python en 2023',\n",
    "             'Qué es Java?', 'Cómo estás?']\n",
    "\n",
    "def procesar_pregunta(consultas):\n",
    "    for consulta in consultas:\n",
    "        # Traemos los documentos más relevantes para la consulta\n",
    "        fuente_contexto = determinar_fuente(consulta, consultas_ejemplo)\n",
    "        print(f'Fuente de contexto seleccionada: {fuente_contexto}')\n",
    "        context_text = contexto_fuente_externa(fuente_contexto, consulta)\n",
    "        final_prompt = None\n",
    "        if not fuente_contexto:\n",
    "            final_prompt = preparar_prompt(\n",
    "                f'Dar la respuesta en el idioma en que fue realizada la pregunta. Responder: {consulta}',\n",
    "                'No hay contexto adicional para esta consulta'\n",
    "            )\n",
    "        elif fuente_contexto == 'csv':\n",
    "            query_str_csv = f'Retorna lo encontrado archivo csv.\\n {consulta}'\n",
    "            final_prompt = preparar_prompt(query_str_csv, str(context_text))\n",
    "        else:\n",
    "            final_prompt = preparar_prompt(consulta, context_text)\n",
    "        print('Pregunta:', consulta)\n",
    "        print('Respuesta:')\n",
    "        answer = generar_respuesta(final_prompt)\n",
    "        if answer:\n",
    "            print(answer)\n",
    "        else:\n",
    "            print('Error al generar la respuesta.')\n",
    "        print('-------------------------------------------------------')\n",
    "\n",
    "# Ejecutamos la función\n",
    "procesar_pregunta(consultas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola! Puedo responder tus preguntas sobre Python (para salir escriba 'q')\n",
      "Chau!\n"
     ]
    }
   ],
   "source": [
    "def chatbot():\n",
    "    '''\n",
    "    Función para interactuar con el bot.\n",
    "    '''\n",
    "\n",
    "    print('Hola! Puedo responder tus preguntas sobre Python (para salir escriba \\'q\\')')\n",
    "\n",
    "    while True:\n",
    "        # Obtener la pregunta del usuario\n",
    "        user_input = input('Usuario: ')\n",
    "\n",
    "        if user_input.lower() == 'q':\n",
    "            print('Chau!')\n",
    "            break\n",
    "\n",
    "        # Responder la pregunta\n",
    "        procesar_pregunta([user_input])\n",
    "\n",
    "chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
